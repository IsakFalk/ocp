{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef18cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from ocpmodels.transfer_learning.common.utils import (\n",
    "    ATOMS_TO_GRAPH_KWARGS,\n",
    "    load_xyz_to_pyg_batch,\n",
    ")\n",
    "from ocpmodels.transfer_learning.loaders import BaseLoader\n",
    "from ocpmodels.transfer_learning.models.distribution_regression import (\n",
    "    EmbeddingKernel,\n",
    "    GaussianKernel,\n",
    "    LinearKernel,\n",
    "    KernelGroupEmbeddingRidgeRegression,\n",
    "    KernelMeanEmbeddingRidgeRegression,\n",
    "    LinearGroupEmbeddingKernel,\n",
    "    LinearMeanEmbeddingKernel,\n",
    "    StandardizedOutputRegression,\n",
    "    median_heuristic,\n",
    ")\n",
    "\n",
    "# from torch_geometric.data.batch import DataBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04303953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/isak/life/references/projects/src/python_lang/ocp\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344e6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import SOAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d2d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isak/life/references/projects/src/python_lang/ocp/ocpmodels/preprocessing/atoms_to_graphs.py:147: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  cell = torch.Tensor(atoms.get_cell()).view(1, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "### Create plot\n",
    "dataset_dir = Path(\"data/private/2_abinitio-metad_Fe45N2.extxyz\")\n",
    "alpha = 1.0\n",
    "aggregation = \"mean\"\n",
    "start_idx = 1600\n",
    "end_idx = -10\n",
    "#lmbda = 1e-9\n",
    "plot_dir = Path(f\"transfer_learning/notebooks/figures/{dataset_dir.stem}/alpha{alpha}_agg{aggregation}_soap\")\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "### Load data\n",
    "raw_data, data_batch, num_frames, num_atoms = load_xyz_to_pyg_batch(dataset_dir, ATOMS_TO_GRAPH_KWARGS[\"schnet\"])\n",
    "\n",
    "soap = SOAP(species=[\"Fe\", \"N\"], r_cut=6, n_max=12, l_max=6, periodic=True, sparse=False)\n",
    "\n",
    "def create_soap_features(systems, soap_object):\n",
    "    features = []\n",
    "    for atoms in systems:\n",
    "        features.append(soap_object.create(atoms))\n",
    "    return torch.tensor(features)\n",
    "soap_features = create_soap_features(raw_data, soap)\n",
    "soap_features = soap_features.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c06d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, n, d = soap_features.shape\n",
    "m = n\n",
    "l = t\n",
    "zx = data_batch.atomic_numbers.reshape(t * n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27208a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_mat(x1, x2, sigma=1.0):\n",
    "    dist = (x1.unsqueeze(1) - x2.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "    return dist\n",
    "\n",
    "def compute_dist_matrix(X, chunk_size=10):\n",
    "    t_n, d = X.shape\n",
    "    D = torch.zeros((t_n, t_n))\n",
    "\n",
    "    for i in range(0, t_n, chunk_size):\n",
    "        i_end = min(i + chunk_size, t_n)\n",
    "        for j in range(0, t_n, chunk_size):\n",
    "            j_end = min(j + chunk_size, t_n)\n",
    "\n",
    "            X_chunk1 = X[i:i_end]\n",
    "            X_chunk2 = X[j:j_end]\n",
    "\n",
    "            D_chunk = distance_mat(X_chunk1, X_chunk2)\n",
    "\n",
    "            D[i:i_end, j:j_end] = D_chunk\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = compute_dist_matrix(soap_features.reshape(t * n, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, sigma=1.0):\n",
    "    dist = (x1.unsqueeze(1) - x2.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "    return torch.exp(-dist / (2 * (sigma**2)))\n",
    "\n",
    "def compute_kernel_matrix(X, chunk_size=100):\n",
    "    t_n, d = X.shape\n",
    "    K = torch.zeros((t_n, t_n))\n",
    "    \n",
    "    for i in range(0, t_n, chunk_size):\n",
    "        i_end = min(i + chunk_size, t_n)\n",
    "        for j in range(0, t_n, chunk_size):\n",
    "            j_end = min(j + chunk_size, t_n)\n",
    "            \n",
    "            X_chunk1 = X[i:i_end]\n",
    "            X_chunk2 = X[j:j_end]\n",
    "            \n",
    "            K_chunk = gaussian_kernel(X_chunk1, X_chunk2)\n",
    "            \n",
    "            K[i:i_end, j:j_end] = K_chunk\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a53c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = (zx[:, None] == zy[None, :]).squeeze()\n",
    "\n",
    "agg_c = 1.0\n",
    "if aggregation == \"mean\":\n",
    "    agg_c /= n * m\n",
    "    # Get all groups and possibly calculate the number\n",
    "    groups = torch.unique(torch.cat([zx, zy]))\n",
    "    group_nx = (zx[:, None] == groups[None, :]).reshape(t, n, -1).sum(axis=1)\n",
    "    group_ny = (zy[:, None] == groups[None, :]).reshape(l, m, -1).sum(axis=1)\n",
    "    select_zx = (zx[:, None] == groups[None, :]).reshape(t, n, -1)\n",
    "    select_zy = (zy[:, None] == groups[None, :]).reshape(l, m, -1)\n",
    "    group_nx_flatten = (select_zx * group_nx[:, None, :]).reshape(t * n, -1).sum(axis=1)\n",
    "    group_ny_flatten = (select_zy * group_ny[:, None, :]).reshape(l * m, -1).sum(axis=1)\n",
    "    # Normalize by the number of kernels in the group \\sum_s K_s / S\n",
    "    mask = (1.0 / group_nx_flatten[:, None]) * (1.0 / group_ny_flatten[None, :])\n",
    "elif self.aggregation == \"sum\":\n",
    "    mask = 1.0\n",
    "else:\n",
    "    raise ValueError(f\"Unknown aggregation {aggregation}\")\n",
    "group_c = mask * delta\n",
    "\n",
    "k = (k0 * ((1 - alpha) * agg_c + alpha * group_c)).reshape(t, n, l, m).sum(axis=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d263bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# set number of clusters\n",
    "n_clusters = 2\n",
    "\n",
    "# perform spectral clustering\n",
    "spec_cluster = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')\n",
    "labels = spec_cluster.fit_predict(K)\n",
    "\n",
    "# plot clustering\n",
    "ts = np.loadtxt(\"./transfer_learning/notebooks/2_abinitio-metad_Fe45N2_N2distance.txt\")[start_idx:end_idx]\n",
    "t = np.arange(ts.shape[0])\n",
    "#K = np.random.rand(100, 100)\n",
    "#labels = np.random.randint(0, 2, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.loadtxt(\"./transfer_learning/notebooks/2_abinitio-metad_Fe45N2_N2distance.txt\")[start_idx:end_idx]\n",
    "t = np.arange(ts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e60597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "\n",
    "#Styling\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.linestyle'] = '-'\n",
    "\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "mpl.rc('mathtext',**{'default':'regular'})\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True\n",
    "})\n",
    "# End\n",
    "\n",
    "heatmap_height = 1\n",
    "ls_ratio = 0.4\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, gridspec_kw={'hspace':0.05})\n",
    "# imgs = []\n",
    "# for i, a in enumerate(ax):\n",
    "#     im = a.imshow(np.random.randn(100,100), cmap='jet', origin='lower')\n",
    "#     imgs.append(im)\n",
    "#     divider = make_axes_locatable(a)\n",
    "#     cax = divider.append_axes('right', size='5%', pad='5%')\n",
    "#     if i <= 3:\n",
    "#         cax.set_axis_off()\n",
    "# cbar = fig.colorbar(imgs[-1], cax=cax)\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1,\n",
    "                       #figsize=(heatmap_height * (1 + ls_ratio), heatmap_height), \n",
    "                       gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\n",
    "\n",
    "# def stylize_axes(ax):\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "\n",
    "#     ax.xaxis.set_tick_params(top='off', direction='out', width=1)\n",
    "#     ax.yaxis.set_tick_params(right='off', direction='out', width=1)\n",
    "\n",
    "#stylize_axes(ax[0])\n",
    "ax[0].plot(ts)\n",
    "cm = sns.color_palette(\"Set2\")\n",
    "eq1 = np.array(labels == 1)\n",
    "ax[0].fill_between(t, 0, 1, where=eq1,\n",
    "                color=cm[0], alpha=0.3, transform=ax[0].get_xaxis_transform())\n",
    "ax[0].fill_between(t, 0, 1, where=np.roll(~eq1, -1) | np.roll(~eq1, 1),\n",
    "                color=cm[1], alpha=0.3, transform=ax[0].get_xaxis_transform())\n",
    "ax[0].set_ylabel(\"$d(\\mathrm{N},\\mathrm{N})$ [\\AA]\")\n",
    "ax[0].yaxis.set_ticks([0, 4.0])\n",
    "ax[0].set_box_aspect(1/3)\n",
    "\n",
    "im = ax[1].imshow(K)\n",
    "ax[1].set_aspect(\"equal\")\n",
    "ax[1].xaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "ax[1].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "ax[1].set_xlabel(\"t\")\n",
    "ax[1].set_ylabel(\"t\")\n",
    "ax[1].set_box_aspect(1)\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "\n",
    "axins = inset_axes(ax[1],\n",
    "                    width=\"5%\",\n",
    "                    height=\"100%\",\n",
    "                    loc='right',\n",
    "                    borderpad=-3)\n",
    "cbar = fig.colorbar(im, cax=axins, orientation=\"vertical\")\n",
    "cbar.set_label(\"Similarity\")\n",
    "fig.savefig(plot_dir / \"spectral_clustering\", dpi=150, bbox_inches='tight')#, transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
