{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Batch, Data\n",
    "import functorch\n",
    "import copy\n",
    "from ocpmodels.transfer_learning.models.distribution_regression import (\n",
    "    GaussianKernel,\n",
    "    KernelMeanEmbeddingRidgeRegression,\n",
    "    LinearMeanEmbeddingKernel,\n",
    "    StandardizedOutputRegression,\n",
    "    median_heuristic,\n",
    ")\n",
    "\n",
    "from ocpmodels.transfer_learning.common.utils import (\n",
    "    ATOMS_TO_GRAPH_KWARGS,\n",
    "    load_xyz_to_pyg_batch,\n",
    "    load_xyz_to_pyg_data,\n",
    ")\n",
    "from ocpmodels.transfer_learning.loaders import BaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd ../..\n",
    "### Load checkpoint\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/s2ef_efwt/all/schnet/schnet_all_large.pt\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "\n",
    "### Load data\n",
    "DATA_PATH = Path(\"data/luigi/example-traj-Fe-N2-111.xyz\")\n",
    "raw_data, data_batch, num_frames, num_atoms = load_xyz_to_pyg_batch(DATA_PATH, ATOMS_TO_GRAPH_KWARGS[\"schnet\"])\n",
    "raw_data, data_list, num_frames, num_atoms = load_xyz_to_pyg_data(DATA_PATH, ATOMS_TO_GRAPH_KWARGS[\"schnet\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "representation_layer = 2\n",
    "base_loader = BaseLoader(\n",
    "    checkpoint[\"config\"],\n",
    "    representation=True,\n",
    "    representation_kwargs={\n",
    "        \"representation_layer\": representation_layer,\n",
    "    },\n",
    ")\n",
    "base_loader.load_checkpoint(CHECKPOINT_PATH, strict_load=False)\n",
    "model = base_loader.model\n",
    "model.to(device)\n",
    "model.mekrr_forces = True\n",
    "model.regress_forces = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x, y):\n",
    "        return x @ y.T\n",
    "\n",
    "lkernel = LinearMeanEmbeddingKernel(LinearKernel())\n",
    "\n",
    "frames = 2\n",
    "dat = Batch.from_data_list(data_batch[:frames]).to(device)\n",
    "dat.pos.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_op(c_0, c_1, dat, kernel):\n",
    "    latent_vars = model(dat)[0]\n",
    "    latent_vars = latent_vars.reshape((-1, num_atoms, latent_vars.shape[-1])).clone()\n",
    "    def model_wrapper(pos, **data):\n",
    "        data[\"pos\"] = pos\n",
    "        data = Data.from_dict(data).to(device)\n",
    "        return model(data)[0]\n",
    "    _dat = copy.deepcopy(dat).to_dict()\n",
    "    pos = _dat.pop(\"pos\")\n",
    "\n",
    "    c_0 = kernel(latent_vars, latent_vars)@c_0\n",
    "    print(c_0.shape)\n",
    "    jvp =  torch.autograd.functional.jvp(lambda x: kernel(model_wrapper(x, **_dat).reshape(-1, num_atoms, latent_vars.shape[-1]), latent_vars), pos, c_1)[1]\n",
    "    print(jvp.shape)\n",
    "    c_1 = c_1\n",
    "    return c_0, c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lin_op(c_0, c_1, dat, lkernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = GaussianKernel()\n",
    "gk.sigma = 1.0\n",
    "gklme = LinearMeanEmbeddingKernel(gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gklme\n",
    "T, n = frames, num_atoms\n",
    "latent_vars = model(dat)\n",
    "d = latent_vars.shape[-1]\n",
    "latent_vars = latent_vars.reshape((T, n, d)).clone().detach()\n",
    "def model_wrapper(pos_tensor, **data):\n",
    "    T, n, _ = pos_tensor.shape\n",
    "    data[\"pos\"] = pos.reshape(-1, 3)\n",
    "    data = Data.from_dict(data).to(device)\n",
    "    return model(data).reshape(T, n, -1)\n",
    "_dat = copy.deepcopy(dat).to_dict()\n",
    "pos = _dat.pop(\"pos\")\n",
    "\n",
    "# Initialize things\n",
    "c0 = torch.zeros(T, 1).to(device)\n",
    "c1 = torch.zeros(T, n, 3).to(device)\n",
    "# Kernel\n",
    "with torch.no_grad():\n",
    "    h = model(dat)\n",
    "    h = h.reshape(T, n, d)\n",
    "    sigma = median_heuristic(h, h)\n",
    "gklme.kernel.sigma = sigma\n",
    "    \n",
    "lmbda = 1e-4\n",
    "k = kernel(h, h) \n",
    "klmbda = (k - k.diag().diag()) + (lmbda + k.diag()).diag()\n",
    "# First part\n",
    "a0 = klmbda @ c0\n",
    "# a0 += /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func = lambda x: kernel(model_wrapper(x, **_dat), latent_vars)\n",
    "def func(x, **_dat):\n",
    "    return model_wrapper(x, **_dat)\n",
    "    #return x.sum()\n",
    "\n",
    "pos_tensor = pos.reshape(T, n, 3)\n",
    "f = lambda x: func(x, **_dat)\n",
    "y = f(pos_tensor)\n",
    "out, jvp =  torch.autograd.functional.jvp(f, pos_tensor, torch.ones_like(pos_tensor), strict=True)\n",
    "print(out)\n",
    "print(jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(y, params={\"pos\": pos_tensor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dat.to_data_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.zeros(T, n, 3).to(device)\n",
    "y_pred = f(pos_tensor)\n",
    "grad_pred = (\n",
    "            torch.autograd.grad(\n",
    "                y_pred,\n",
    "                pos,\n",
    "                grad_outputs=torch.ones_like(y_pred),\n",
    "                create_graph=False,\n",
    "                allow_unused=False,\n",
    "            )[0]\n",
    ")\n",
    "grad_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### _c1 = torch.zeros(T, n, 3)\n",
    "out_dict = {\"out\": [], \"jvp\": []}\n",
    "for t in range(T):\n",
    "    _dat = dat[t].to_dict()\n",
    "    pos = _dat.pop(\"pos\")\n",
    "    func = lambda x: kernel(model_wrapper(x, **_dat).reshape(1, n, d), latent_vars)\n",
    "    out, jvp =  torch.autograd.functional.jvp(func, pos[t], c1[t].reshape(1, n, 3))\n",
    "    out_dict[\"out\"].append(out)\n",
    "    out_dict[\"jvp\"].append(jvp)\n",
    "    \n",
    "out = torch.cat(out_dict[\"out\"])\n",
    "jvp = torch.cat(out_dict[\"jvp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jvp =  torch.autograd.functional.jvp(lambda x: kernel(model_wrapper(x, **_dat).reshape(-1, num_atoms, latent_vars.shape[-1]), latent_vars), pos, c_1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos.shape, func(pos).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(pos, data, model, kernel):\n",
    "    h = model(data)[0]\n",
    "    h_ = h.clone().detach()\n",
    "    return kernel(h, h_)\n",
    "\n",
    "\n",
    "y = f(\n",
    "    pos,\n",
    "    dat,\n",
    "    model,\n",
    "    lkernel,\n",
    ")\n",
    "m = y.shape[0]\n",
    "gr = torch.autograd.grad(\n",
    "    outputs=y,\n",
    "    inputs=pos,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "    allow_unused=False,\n",
    "    is_grads_batched=False,\n",
    ")[0]\n",
    "pos.shape\n",
    "gr.shape\n",
    "\n",
    "output, vjp_fn = torch.func.vjp(lambda x: f(x, dat, model, lkernel), pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check that we can get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(pos, data, model, kernel):\n",
    "    h = model(data)[0]\n",
    "    h_ = h.clone().detach()\n",
    "    return kernel(h, h_)\n",
    "\n",
    "\n",
    "y = f(\n",
    "    pos,\n",
    "    dat,\n",
    "    model,\n",
    "    lkernel,\n",
    ")\n",
    "m = y.shape[0]\n",
    "gr = torch.autograd.grad(\n",
    "    outputs=y,\n",
    "    inputs=pos,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "    allow_unused=False,\n",
    "    is_grads_batched=False,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = model(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(y, x, grad_outputs=None):\n",
    "    \"\"\"Compute dy/dx @ grad_outputs\"\"\"\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs = grad_outputs, create_graph=True)[0]\n",
    "    return grad\n",
    "\n",
    "def jacobian(y, x):\n",
    "    \"\"\"Compute dy/dx = dy/dx @ grad_outputs; \n",
    "    for grad_outputs in[1, 0, ..., 0], [0, 1, 0, ..., 0], ...., [0, ..., 0, 1]\"\"\"\n",
    "    jac = torch.zeros(y.shape[0], x.shape[0]) \n",
    "    for i in range(y.shape[0]):\n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[i] = 1\n",
    "        jac[i] = gradient(y, x, grad_outputs = grad_outputs)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacobian(phi, dat.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f(pos, data, model):\n",
    "    h = model(data).reshape(frames, num_atoms, d)\n",
    "    return h\n",
    "\n",
    "\n",
    "y = f(\n",
    "    dat.pos,\n",
    "    dat,\n",
    "    model,\n",
    ")\n",
    "m = y.shape[0]\n",
    "gr = torch.autograd.grad(\n",
    "    outputs=y,\n",
    "    inputs=dat.pos,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "    allow_unused=False,\n",
    "    is_grads_batched=False,\n",
    ")[0]\n",
    "\n",
    "output, vjp_fn = torch.func.vjp(lambda x: f(x, dat, model), dat.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(y, x, grad_outputs=None):\n",
    "    \"\"\"Compute dy/dx @ grad_outputs\"\"\"\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs = grad_outputs, create_graph=True)[0]\n",
    "    return grad\n",
    "\n",
    "def jacobian(y, x):\n",
    "    \"\"\"Compute dy/dx = dy/dx @ grad_outputs; \n",
    "    for grad_outputs in[1, 0, ..., 0], [0, 1, 0, ..., 0], ...., [0, ..., 0, 1]\"\"\"\n",
    "    jac = torch.zeros(y.shape[0], x.shape[0]) \n",
    "    for i in range(y.shape[0]):\n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[i] = 1\n",
    "        jac[i] = gradient(y, x, grad_outputs = grad_outputs)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_dict = dat.to_dict()\n",
    "print(dat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = list(dat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(pos, cell, atomic_numbers, natoms, tags, edge_index, cell_offsets, y, force, fixed, batch, ptr, model):\n",
    "    data = Data.from_dict({\n",
    "        \"pos\": pos,\n",
    "        \"cell\": cell,\n",
    "        \"atomic_numbers\": atomic_numbers,\n",
    "        \"natoms\": natoms, \n",
    "        \"tags\": tags,\n",
    "        \"edge_index\": edge_index, \n",
    "        \"cell_offsets\": cell_offsets,\n",
    "        \"y\": y,\n",
    "        \"force\": force,\n",
    "        \"fixed\": fixed,\n",
    "        \"batch\": batch,\n",
    "        \"ptr\": ptr\n",
    "    })\n",
    "    h = model(data)\n",
    "    return h\n",
    "\n",
    "dat_dict = dat.to_dict()\n",
    "pos = dat_dict.pop(\"pos\")\n",
    "y = f(pos=pos, **dat_dict, model=model)\n",
    "\n",
    "gr = torch.autograd.grad(\n",
    "    outputs=y,\n",
    "    inputs=pos,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "    allow_unused=False,\n",
    "    is_grads_batched=False,\n",
    ")[0]\n",
    "\n",
    "func_output, jvp = torch.autograd.functional.jvp(lambda x: f(x, **dat_dict, model=model),\n",
    "                                                 pos,\n",
    "                                                 v=torch.ones_like(pos),\n",
    "                                                 strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(pos, , model):\n",
    "    h = model(data).reshape(-1)\n",
    "    return h\n",
    "\n",
    "y = f(\n",
    "    dat.pos,\n",
    "    dat,\n",
    "    model,\n",
    ")\n",
    "jac = jacobian(y, dat.pos.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.common.utils import get_max_neighbors_mask, get_pbc_distances, radius_graph_pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_scatter import scatter, segment_coo, segment_csr\n",
    "import torch\n",
    "import torch_scatter\n",
    "\n",
    "# For the CSR operator:\n",
    "class SumCSR(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, groups):\n",
    "        ctx.save_for_backward(groups)\n",
    "        return torch_scatter.segment_csr(values, groups, reduce=\"sum\")\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (groups,) = ctx.saved_tensors\n",
    "        return GatherCSR.apply(grad_output, groups), None\n",
    "\n",
    "\n",
    "class GatherCSR(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, groups):\n",
    "        ctx.save_for_backward(groups)\n",
    "        return torch_scatter.gather_csr(values, groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (groups,) = ctx.saved_tensors\n",
    "        return SumCSR.apply(grad_output, groups), None\n",
    "\n",
    "\n",
    "# For the COO operator:\n",
    "class SumCOO(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, groups, dim_size):\n",
    "        ctx.save_for_backward(groups)\n",
    "        ctx.dim_size = dim_size\n",
    "        return torch_scatter.segment_coo(\n",
    "            values, groups, dim_size=dim_size, reduce=\"sum\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (groups,) = ctx.saved_tensors\n",
    "        return GatherCOO.apply(grad_output, groups, ctx.dim_size), None, None\n",
    "\n",
    "\n",
    "class GatherCOO(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, groups, dim_size):\n",
    "        ctx.save_for_backward(groups)\n",
    "        ctx.dim_size = dim_size\n",
    "        return torch_scatter.gather_coo(values, groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (groups,) = ctx.saved_tensors\n",
    "        return SumCOO.apply(grad_output, groups, ctx.dim_size), None, None\n",
    "\n",
    "def radius_graph_pbc(data, radius, max_num_neighbors_threshold, pbc=[True, True, True]):\n",
    "    device = data.pos.device\n",
    "    batch_size = len(data.natoms)\n",
    "\n",
    "    if hasattr(data, \"pbc\"):\n",
    "        data.pbc = torch.atleast_2d(data.pbc)\n",
    "        for i in range(3):\n",
    "            if not torch.any(data.pbc[:, i]).item():\n",
    "                pbc[i] = False\n",
    "            elif torch.all(data.pbc[:, i]).item():\n",
    "                pbc[i] = True\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    \"Different structures in the batch have different PBC configurations. This is not currently supported.\"\n",
    "                )\n",
    "\n",
    "    # position of the atoms\n",
    "    atom_pos = data.pos\n",
    "\n",
    "    # Before computing the pairwise distances between atoms, first create a list of atom indices to compare for the entire batch\n",
    "    num_atoms_per_image = data.natoms\n",
    "    num_atoms_per_image_sqr = (num_atoms_per_image**2).long()\n",
    "\n",
    "    # index offset between images\n",
    "    index_offset = torch.cumsum(num_atoms_per_image, dim=0) - num_atoms_per_image\n",
    "\n",
    "    index_offset_expand = torch.repeat_interleave(index_offset, num_atoms_per_image_sqr)\n",
    "    num_atoms_per_image_expand = torch.repeat_interleave(num_atoms_per_image, num_atoms_per_image_sqr)\n",
    "\n",
    "    # Compute a tensor containing sequences of numbers that range from 0 to num_atoms_per_image_sqr for each image\n",
    "    # that is used to compute indices for the pairs of atoms. This is a very convoluted way to implement\n",
    "    # the following (but 10x faster since it removes the for loop)\n",
    "    # for batch_idx in range(batch_size):\n",
    "    #    batch_count = torch.cat([batch_count, torch.arange(num_atoms_per_image_sqr[batch_idx], device=device)], dim=0)\n",
    "    num_atom_pairs = torch.sum(num_atoms_per_image_sqr)\n",
    "    index_sqr_offset = torch.cumsum(num_atoms_per_image_sqr, dim=0) - num_atoms_per_image_sqr\n",
    "    index_sqr_offset = torch.repeat_interleave(index_sqr_offset, num_atoms_per_image_sqr)\n",
    "    atom_count_sqr = torch.arange(num_atom_pairs, device=device) - index_sqr_offset\n",
    "\n",
    "    # Compute the indices for the pairs of atoms (using division and mod)\n",
    "    # If the systems get too large this apporach could run into numerical precision issues\n",
    "    index1 = (torch.div(atom_count_sqr, num_atoms_per_image_expand, rounding_mode=\"floor\")) + index_offset_expand\n",
    "    index2 = (atom_count_sqr % num_atoms_per_image_expand) + index_offset_expand\n",
    "    # Get the positions for each atom\n",
    "    pos1 = torch.index_select(atom_pos, 0, index1)\n",
    "    pos2 = torch.index_select(atom_pos, 0, index2)\n",
    "\n",
    "    # Calculate required number of unit cells in each direction.\n",
    "    # Smallest distance between planes separated by a1 is\n",
    "    # 1 / ||(a2 x a3) / V||_2, since a2 x a3 is the area of the plane.\n",
    "    # Note that the unit cell volume V = a1 * (a2 x a3) and that\n",
    "    # (a2 x a3) / V is also the reciprocal primitive vector\n",
    "    # (crystallographer's definition).\n",
    "\n",
    "    cross_a2a3 = torch.cross(data.cell[:, 1], data.cell[:, 2], dim=-1)\n",
    "    cell_vol = torch.sum(data.cell[:, 0] * cross_a2a3, dim=-1, keepdim=True)\n",
    "\n",
    "    if pbc[0]:\n",
    "        inv_min_dist_a1 = torch.norm(cross_a2a3 / cell_vol, p=2, dim=-1)\n",
    "        rep_a1 = torch.ceil(radius * inv_min_dist_a1)\n",
    "    else:\n",
    "        rep_a1 = data.cell.new_zeros(1)\n",
    "\n",
    "    if pbc[1]:\n",
    "        cross_a3a1 = torch.cross(data.cell[:, 2], data.cell[:, 0], dim=-1)\n",
    "        inv_min_dist_a2 = torch.norm(cross_a3a1 / cell_vol, p=2, dim=-1)\n",
    "        rep_a2 = torch.ceil(radius * inv_min_dist_a2)\n",
    "    else:\n",
    "        rep_a2 = data.cell.new_zeros(1)\n",
    "\n",
    "    if pbc[2]:\n",
    "        cross_a1a2 = torch.cross(data.cell[:, 0], data.cell[:, 1], dim=-1)\n",
    "        inv_min_dist_a3 = torch.norm(cross_a1a2 / cell_vol, p=2, dim=-1)\n",
    "        rep_a3 = torch.ceil(radius * inv_min_dist_a3)\n",
    "    else:\n",
    "        rep_a3 = data.cell.new_zeros(1)\n",
    "\n",
    "    # Take the max over all images for uniformity. This is essentially padding.\n",
    "    # Note that this can significantly increase the number of computed distances\n",
    "    # if the required repetitions are very different between images\n",
    "    # (which they usually are). Changing this to sparse (scatter) operations\n",
    "    # might be worth the effort if this function becomes a bottleneck.\n",
    "    max_rep = [rep_a1.max(), rep_a2.max(), rep_a3.max()]\n",
    "\n",
    "    # Tensor of unit cells\n",
    "    cells_per_dim = [torch.arange(-rep, rep + 1, device=device, dtype=torch.float) for rep in max_rep]\n",
    "    unit_cell = torch.cartesian_prod(*cells_per_dim)\n",
    "    num_cells = len(unit_cell)\n",
    "    unit_cell_per_atom = unit_cell.view(1, num_cells, 3).repeat(len(index2), 1, 1)\n",
    "    unit_cell = torch.transpose(unit_cell, 0, 1)\n",
    "    unit_cell_batch = unit_cell.view(1, 3, num_cells).expand(batch_size, -1, -1)\n",
    "\n",
    "    # Compute the x, y, z positional offsets for each cell in each image\n",
    "    data_cell = torch.transpose(data.cell, 1, 2)\n",
    "    pbc_offsets = torch.bmm(data_cell, unit_cell_batch)\n",
    "    pbc_offsets_per_atom = torch.repeat_interleave(pbc_offsets, num_atoms_per_image_sqr, dim=0)\n",
    "\n",
    "    # Expand the positions and indices for the 9 cells\n",
    "    pos1 = pos1.view(-1, 3, 1).expand(-1, -1, num_cells)\n",
    "    pos2 = pos2.view(-1, 3, 1).expand(-1, -1, num_cells)\n",
    "    index1 = index1.view(-1, 1).repeat(1, num_cells).view(-1)\n",
    "    index2 = index2.view(-1, 1).repeat(1, num_cells).view(-1)\n",
    "    # Add the PBC offsets for the second atom\n",
    "    pos2 = pos2 + pbc_offsets_per_atom\n",
    "\n",
    "    # Compute the squared distance between atoms\n",
    "    atom_distance_sqr = torch.sum((pos1 - pos2) ** 2, dim=1)\n",
    "    atom_distance_sqr = atom_distance_sqr.view(-1)\n",
    "\n",
    "    # Remove pairs that are too far apart\n",
    "    mask_within_radius = torch.le(atom_distance_sqr, radius * radius)\n",
    "    # Remove pairs with the same atoms (distance = 0.0)\n",
    "    mask_not_same = torch.gt(atom_distance_sqr, 0.0001)\n",
    "    mask = torch.logical_and(mask_within_radius, mask_not_same)\n",
    "    index1 = torch.masked_select(index1, mask)\n",
    "    index2 = torch.masked_select(index2, mask)\n",
    "    unit_cell = torch.masked_select(unit_cell_per_atom.view(-1, 3), mask.view(-1, 1).expand(-1, 3))\n",
    "    unit_cell = unit_cell.view(-1, 3)\n",
    "    atom_distance_sqr = torch.masked_select(atom_distance_sqr, mask)\n",
    "\n",
    "    # Remove due to errors with jvps\n",
    "#     mask_num_neighbors, num_neighbors_image = get_max_neighbors_mask(\n",
    "#         natoms=data.natoms,\n",
    "#         index=index1,\n",
    "#         atom_distance=atom_distance_sqr,\n",
    "#         max_num_neighbors_threshold=max_num_neighbors_threshold,\n",
    "#     )\n",
    "    num_neighbors_image = torch.tensor(len(index1)).to(device)\n",
    "\n",
    "#     if not torch.all(mask_num_neighbors):\n",
    "#         # Mask out the atoms to ensure each atom has at most max_num_neighbors_threshold neighbors\n",
    "#         index1 = torch.masked_select(index1, mask_num_neighbors)\n",
    "#         index2 = torch.masked_select(index2, mask_num_neighbors)\n",
    "#         unit_cell = torch.masked_select(unit_cell.view(-1, 3), mask_num_neighbors.view(-1, 1).expand(-1, 3))\n",
    "#         unit_cell = unit_cell.view(-1, 3)\n",
    "    unit_cell = unit_cell.view(-1, 3)\n",
    "    edge_index = torch.stack((index2, index1))\n",
    "\n",
    "    return edge_index, unit_cell, num_neighbors_image\n",
    "\n",
    "def get_max_neighbors_mask(natoms, index, atom_distance, max_num_neighbors_threshold):\n",
    "    \"\"\"\n",
    "    Give a mask that filters out edges so that each atom has at most\n",
    "    `max_num_neighbors_threshold` neighbors.\n",
    "    Assumes that `index` is sorted.\n",
    "    \"\"\"\n",
    "    device = natoms.device\n",
    "    num_atoms = natoms.sum()\n",
    "\n",
    "    # Get number of neighbors\n",
    "    # segment_coo assumes sorted index\n",
    "    ones = index.new_ones(1).expand_as(index)\n",
    "    num_neighbors = segment_coo(ones, index, dim_size=num_atoms)\n",
    "    max_num_neighbors = num_neighbors.max()\n",
    "    num_neighbors_thresholded = num_neighbors.clamp(max=max_num_neighbors_threshold)\n",
    "\n",
    "    # Get number of (thresholded) neighbors per image\n",
    "    image_indptr = torch.zeros(natoms.shape[0] + 1, device=device, dtype=torch.long)\n",
    "    image_indptr[1:] = torch.cumsum(natoms, dim=0)\n",
    "    num_neighbors_image = segment_csr(num_neighbors_thresholded, image_indptr)\n",
    "\n",
    "    # If max_num_neighbors is below the threshold, return early\n",
    "    if max_num_neighbors <= max_num_neighbors_threshold or max_num_neighbors_threshold <= 0:\n",
    "        mask_num_neighbors = torch.tensor([True], dtype=bool, device=device).expand_as(index)\n",
    "        return mask_num_neighbors, num_neighbors_image\n",
    "\n",
    "    # Create a tensor of size [num_atoms, max_num_neighbors] to sort the distances of the neighbors.\n",
    "    # Fill with infinity so we can easily remove unused distances later.\n",
    "    distance_sort = torch.full([num_atoms * max_num_neighbors], np.inf, device=device)\n",
    "\n",
    "    # Create an index map to map distances from atom_distance to distance_sort\n",
    "    # index_sort_map assumes index to be sorted\n",
    "    index_neighbor_offset = torch.cumsum(num_neighbors, dim=0) - num_neighbors\n",
    "    index_neighbor_offset_expand = torch.repeat_interleave(index_neighbor_offset, num_neighbors)\n",
    "    index_sort_map = index * max_num_neighbors + torch.arange(len(index), device=device) - index_neighbor_offset_expand\n",
    "    distance_sort.index_copy_(0, index_sort_map, atom_distance)\n",
    "    distance_sort = distance_sort.view(num_atoms, max_num_neighbors)\n",
    "\n",
    "    # Sort neighboring atoms based on distance\n",
    "    distance_sort, index_sort = torch.sort(distance_sort, dim=1)\n",
    "    # Select the max_num_neighbors_threshold neighbors that are closest\n",
    "    distance_sort = distance_sort[:, :max_num_neighbors_threshold]\n",
    "    index_sort = index_sort[:, :max_num_neighbors_threshold]\n",
    "\n",
    "    # Offset index_sort so that it indexes into index\n",
    "    index_sort = index_sort + index_neighbor_offset.view(-1, 1).expand(-1, max_num_neighbors_threshold)\n",
    "    # Remove \"unused pairs\" with infinite distances\n",
    "    mask_finite = torch.isfinite(distance_sort)\n",
    "    index_sort = torch.masked_select(index_sort, mask_finite)\n",
    "\n",
    "    # At this point index_sort contains the index into index of the\n",
    "    # closest max_num_neighbors_threshold neighbors per atom\n",
    "    # Create a mask to remove all pairs not in index_sort\n",
    "    mask_num_neighbors = torch.zeros(len(index), device=device, dtype=bool)\n",
    "    mask_num_neighbors.index_fill_(0, index_sort, True)\n",
    "\n",
    "    return mask_num_neighbors, num_neighbors_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Batch.from_data_list(data_batch[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, cell_offsets, neighbors = radius_graph_pbc(data, 6.0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_pbc_distances(\n",
    "    data.pos,\n",
    "    edge_index,\n",
    "    data.cell,\n",
    "    cell_offsets,\n",
    "    neighbors,\n",
    "    return_offsets=True,\n",
    "    return_distance_vec=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_offsets, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.common.utils import get_max_neighbors_mask, get_pbc_distances, radius_graph_pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1, cell_offsets1, neighbors1 = radius_graph_pbc(data, 6.0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = get_pbc_distances(\n",
    "    data.pos,\n",
    "    edge_index1,\n",
    "    data.cell,\n",
    "    cell_offsets1,\n",
    "    neighbors1,\n",
    "    return_offsets=True,\n",
    "    return_distance_vec=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(out[\"edge_index\"], out1[\"edge_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in out.items():\n",
    "    print(key)\n",
    "    print(torch.equal(val, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
